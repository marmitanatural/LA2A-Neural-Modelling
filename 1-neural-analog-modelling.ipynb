{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Analog Modelling \n",
    "Please note that the dataset used for this project is not 100% clean, and the errors from the authors errata page are addressed and resolved in the other notebook `0-data-validation.ipynb`. This is done to ensure that in the below our models are not impacted.\n",
    "### -- Introduction\n",
    "This repo is concerned with modelling analog effects with neural networks. Why? Mostly for curiosity.\n",
    "\n",
    "Most VST effects are typically implemented in C++, using handy frameworks like JUCE which contains libraries to handle many of the typical challenges of plugin design such as cross-platform functionality, DSP, frontend design.. In order to do real analog modelling, one typically needs to have some high level domain knowledge and have the capacity to understand complicated analog circuits present in analog synths and effects, as well as a knowledge of DSP which allows us to model these signals numerically. Other approaches include simulation of physical processes like reverberation.\n",
    "\n",
    "In principle, neural networks are a more high level approach which, presupposing the access to a relevant dataset of dry/wet signals, allow us to model the effects without having to bust out Korg manuals from the 1980s and examine the intricacies of the circuits for their wonderful filters. \n",
    "\n",
    "There are clearly many limits in using neural networks to process audio. Some include:\n",
    "- Clean datasets of dry/wet signals are not typically easy to come by, and are labour intensive to repair. Hence NNs are not always a suitable approach for modelling analog effects.\n",
    "- Neural networks are typically quite bulky, and implemented in Python (which is typically slower compared to DSP implementations in C++). Although not impossible (see work of [C. Steinmetz](https://scholar.google.com/citations?user=jSvSfIMAAAAJ&hl=en)), this makes it difficult to use neural networks to process signals in real time. This inherent slowness makes a lot of neural approaches only suitable to asynchronous audio processing. Sequence to sequence audio modelling is notoriously slow, especially given the large size of common audio models used today e.g [Demucs](https://github.com/facebookresearch/demucs) (for source separation). \n",
    "\n",
    "I am interesting in exploring these limits, especially the second one. \n",
    "\n",
    "### -- Data \n",
    "In this case, we use the [SignalTrain](https://zenodo.org/records/3824876) dataset from 2019 which contains various dry and wet recordings, where the wet recordings are processed with an analog compressor, the Universal Audio LA-2A. \n",
    "\n",
    "This compressor is a very simple one, and we will be concerned with modelling the signal using only two parameters on the compressor:\n",
    "- The switch between compression and limiting\n",
    "- The peak reduction\n",
    "\n",
    "The information about the parameters of the compressor are contained in the file names. In particular, the value between 0-100 represents the peak reduction knob, whereas the binary value 0/1 represents the switch between compression (0) and limiting (1). The authors say that there was no changes to the input or output gains, and that only these two parameters above were changed during recording.\n",
    "\n",
    "### -- Audio\n",
    "The audio in this dataset has a sampling rate of 44.1kHz and is mono, as the original analog LA-2A was designed to process mono signals. The individual audio files actually contain a \"collage\" of different pieces of music which are stitched seamlessly together. The whole recording will be passed through the compressor to obtain the wet signal.\n",
    "\n",
    "I followed the advice of the authors in cleaning up errors in the dataset (removing / moving certain files) in other notebook `data_validation.ipynb`. I cross correlated all the signals to see if any signals except those mentioned by the authors had a phase shift between the dry and wet signals. To simplify my life, I found all the signals which had a relative phase shift and removed them, keeping only perfectly correlated dry/ wet pairs; which to the credit of the authors, was the vast majority of the signals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compressors\n",
    "\n",
    "In the context of music production we are interested in Dynamic Range Compressors. These are effects which reduce the dynamic range (the gap between the loudest and quietest parts of a signal) via downward or upward compression (reducing the gain of loud sounds, and increasing the gain of quiet sounds respectively). They have several parameters, the most important ones being:\n",
    "\n",
    "- Threshold: the volume at which the compressor will be activated. In the case of downward compression, if the threshold is -6dB, and the signal peaks at -10dB, the compressor will never be activated, if it is, the compressor will activate and trigger gain reduction.  On the other hand, in upward compression, if the threshold is -6dB, any sound below this threshold (e.g -10dB) will activate the compression and cause a gain increase.\n",
    "\n",
    "- Ratio: a parameter which controls the amount of compression applied to an incoming signal. It is called a ratio because it is typically defined in such a way that if the ratio is 4:1, any signal 4dB **over** the threshold will be reduced to 1dB over the threshold.\n",
    "\n",
    "Some other common parameters include attack and release, which delay or extend resp. the activation of the compressor. e.g if the attack is 50ms, when the compressor detects a signal which crosses the threshold, it will delay its activation by 50ms. The release, if 50ms, will extend the action of the compressor by 50ms. The effect of attack / release is often not considered in a step function like manner, but will often be smoother, where for example during the attack phase of 50ms after the detection of a signal crossing the compressor's threshold, the action of the compressor may be linearly ramped up to its full action. \n",
    "\n",
    "Another important control is the \"knee\" setting. In the case of a \"hard knee\" compressor, the compression will only activate once the signal crosses the threshold, triggering gain reduction. In the case of a soft knee compressor, this transition point is 'blurred' and less abrupt, even for signals below the threshold, the gain may be attenuated slightly by the compressor. This results in a 'smoother' transition between compressor and uncompressed parts of the signal. \n",
    "\n",
    "Here is an example of a minimal compressor implemented in numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compressor(signal, threshold, ratio):\n",
    "    compressed_signal = np.zeros_like(signal)\n",
    "    for i in range(len(signal)):\n",
    "        if np.abs(signal[i]) > threshold:\n",
    "            compressed_signal[i] = np.sign(signal[i]) * (threshold + (np.abs(signal[i]) - threshold) / ratio)\n",
    "        else:\n",
    "            compressed_signal[i] = signal[i]\n",
    "    return compressed_signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Something important to remember is the units in which you are working. In a 16 bit system, a sample can take any value between -32768 and +32767 (2^16 values hence 16 \"bit\"). If a signal exceeds this maximum value, it may result in clipping (the value of the signal will be cut off at the maximum value). From these raw numbers, we can define the dBFS units (decibels relative to full scale), where here our \"full scale\" value will be the maximum value of our amplitude in our 16 bit system, +32767. The conversion can be defined as $ X \\text(dB) = 20 \\text{log}_{10} \\left( \\frac{\\text{Amplitude}}{32767} \\right)$. If we reach our max value, notice that the corresponding value in dBFS will be 0dB, as expected. \n",
    "\n",
    "dB is commonly used because humans have a logarithmic perception of volume - i.e doubling the intensity of a signal will not be perceived by humans as \"twice as loud\" indeed the human perception of loudness more closely mimics that of the logarithm of intensity, rather than a linear relationship - interestingly, loudness is a \"psychological\" quantity, and humans even have different perceptions of loudness depending on the frequency of the incoming signal - that means that a signal at 2kHz and 15kHz with the same intensity will not be perceived as the same loudness by a human. For further details you can read about Fletcher-Munson curves which discuss this phenomenon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata Preparation \n",
    "It is common when working with data such as audio to keep a metadata dataframe which contains important information about the audio that is not contained in the raw audio signal, so that we can feed this information to our model. Whether compression or limiting is being applied is an example of such metadata. The authors did not include such a metadata frame i.e a df with each row being a track, with a unique id, the paths to the raw and processed audio, and the compression settings applied, so we have to create it ourselves from the filenames provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data consists of some raw audio files of the form e.g `./data/<split>/input_XXX_.wav` and some corresponding processed audio files `./data/<split>.target_XXX_LA2A_YY__Z__WW.wav`. The compression parameters are contained in the file name of the processed file. The parameters are the following:\n",
    "- XXX: audio file id\n",
    "- YY: seems to be the compressor revision, not that important\n",
    "- Z: compressor/limiter switch either 0 or 1. \n",
    "- WW: peak reduction switch, from 0-100. \n",
    "\n",
    "In the other notebook I already removed pairs of audio which were out of phase (an error in the creation of the dataset), so we can proceed with extracting the data from the track files as described above. \n",
    "\n",
    " The input data will be:\n",
    "- Raw audio segment.\n",
    "- Compressor / Limiter Switch (0 or 1 resp.).\n",
    "- Compressor peak reduction (0 - 100).\n",
    "Where the target will be the corresponding processed audio segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of unique track ids by parsing the info in the file names\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def get_track_ids(track_paths):\n",
    "    \"\"\"\n",
    "    Gets a list of track_ids which are unique from the given track_paths\n",
    "    \"\"\"\n",
    "    track_ids = [track_path.split(\"_\")[1] for track_path in track_paths]\n",
    "    numeric_ids_only = [track for track in track_ids if track.isdigit()]\n",
    "    return list(set(numeric_ids_only))\n",
    "\n",
    "\n",
    "def prepare_metadata_records(splits):\n",
    "    \"\"\"\n",
    "    Gets a set of parsed metadata records for every track in each split.\n",
    "    - Creates a dict with split names as keys e.g {'train': .., 'test': .., ..}\n",
    "    - The value corresponding to each split name is a list of nested records of the form [{'track_id':{'X_path':xxx, 'param1':yyy, 'param2':zzz, 'Y_path':www}, ..]\n",
    "    \"\"\"\n",
    "    metadata = defaultdict(list)\n",
    "    for split in splits:\n",
    "        split_name = split.split(\"/\")[-2].lower()  # unfortunate naming of variable here\n",
    "        track_paths = os.listdir(split)\n",
    "        track_ids = get_track_ids(track_paths)\n",
    "\n",
    "        split_metadata = []\n",
    "        for track_id in track_ids:\n",
    "            track_level_data = defaultdict(dict)\n",
    "            for track_path in track_paths:\n",
    "                if track_id in track_path and \"target\" in track_path:\n",
    "                    split_path = track_path.split(\"_\")\n",
    "                    compress_or_limit = split_path[-3]\n",
    "                    peak_reduction = split_path[-1].split(\".wav\")[0]\n",
    "\n",
    "                    track_level_data[track_id][\"raw_audio_path\"] = (\n",
    "                        split + \"input_\" + track_id + \"_.wav\"\n",
    "                    )\n",
    "                    track_level_data[track_id][\"compress_or_limit\"] = compress_or_limit\n",
    "                    track_level_data[track_id][\"peak_reduction\"] = peak_reduction\n",
    "                    track_level_data[track_id][\"processed_audio_path\"] = (\n",
    "                        split + track_path\n",
    "                    )\n",
    "            split_metadata.append(track_level_data)\n",
    "        metadata[split_name] = split_metadata\n",
    "    return metadata\n",
    "\n",
    "\n",
    "def turn_records_into_df(records):\n",
    "    \"\"\"\n",
    "    Turns the metadata records into a nice and simple dataframe\n",
    "    \"\"\"\n",
    "    flattened_data = []\n",
    "    for split, records in records.items():\n",
    "        for record in records:\n",
    "            for record_id, details in record.items():\n",
    "                flattened_data.append(\n",
    "                    {\"split\": split, \"track_id\": record_id, **details}\n",
    "                )\n",
    "\n",
    "    return pd.DataFrame(flattened_data)\n",
    "\n",
    "\n",
    "splits = [\"./data/Train/\", \"./data/Test/\", \"./data/Val/\"]\n",
    "records = prepare_metadata_records(splits)\n",
    "df = turn_records_into_df(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our basic metadata df that will allow us to prepare each small audio segment for training, let's just do a quick sanity check to make sure we have no parsing errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "split                   0\n",
       "track_id                0\n",
       "raw_audio_path          0\n",
       "compress_or_limit       0\n",
       "peak_reduction          0\n",
       "processed_audio_path    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.track_id.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good, no NaNs or duplicates. Let's just fix the types of some columns as our model will need them to have the correct types later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"track_id\"] = df[\"track_id\"].astype(int)\n",
    "df[\"compress_or_limit\"] = df[\"compress_or_limit\"].astype(int)\n",
    "df[\"peak_reduction\"] = df[\"peak_reduction\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Audio Segments for Training + Evaluation + Challenges\n",
    "Each file in the dataset is not a unique track, but rather a very long (can be up to 20 minutes) collage of different tracks, stitched together without interruption. All tracks in the file are compressed with the same compression parameters indicated in the \"target\" file name. We also have access to the raw, uncompressed audio file corresponding to this.\n",
    "\n",
    "Our model will not have an input size of 20 minutes and will not handle variable input sizes effectively, so we need to break each file into chunks of 3-10 seconds and process them one by one, these are common input sizes in the literature for audio ML (e.g ShortChunk has 15 seconds, whereas MusiCNN has 3 seconds). \n",
    "\n",
    "This means that from 1 file of 20 minutes, we will actually many training examples for a model with 1 second input length. \n",
    "\n",
    "It also means that during inference (i.e during modelling) we will not be able to process a whole file at once but will need to split the incoming audio into 3 second chunks and then process each chunk separately. We will call the incoming audio chunk the \"buffer\". This fact is actually a serious design challenge for modelling a time based effect such as compression, because a naive model architecture means it will only use incoming audio in the buffer to produce an output - but time based effects like compression have parameters such as attack/release (discussed above) which means audio in the buffer should trigger compression in the next incoming buffer. If this is not clear, imagine that our 3 second audio signal in the buffer has a loud peak at 2.99s. If our compressor has an attack time of 0,015s (15ms), the compression will only \"kick in\" or be triggered _after_ the current signal is out of the buffer (2.99 + 0.15 > 3.0), affecting only the start of the next 3 second signal coming into the buffer. This means there is a potential dependency between windows that are being processed independently by our model. The same problem exists in the realm of DSP audio plugin design, where these \"transient\" errors are often treated using a lookahead buffer. This challenge may end up motivating the design of our model later if naive models prove to suffer from this possible complication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will load a full 15-20 minute audio file and split it up into windows of size 1 seconds. We will have a window stride of 0.5 second. This means that we will keep a bit of the \"last\" window in the current window, which is a common technique in audio processing / time series analysis to have smoother transitions between adjacent windows. Let's make a helper function that will take an object of size N, a window of size K (K <= N) and a stride length S and compute how many overlapping intervals we can compute from our audio. For an example of what this means look below for an explicit example."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "- - - - -\n",
    "\n",
    "- - \n",
    "  - -\n",
    "    - -\n",
    "      - -\n",
    "\n",
    "-> 4 overlapping intervals of size 2 and stride 1 fit in object of length 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlapping_interval_count(object_size, window_size, window_stride):\n",
    "    \"\"\"all lengths in samples here\"\"\"\n",
    "    stride = 0\n",
    "    count = 0\n",
    "    while True:\n",
    "        pos = stride + window_size \n",
    "        if pos >= object_size:\n",
    "            return count + 1 # gives number of overlapping intervals that fit in the \n",
    "        count += 1\n",
    "        stride += window_stride"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create our training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "import torch \n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_audio_paths,\n",
    "        target_audio_paths,\n",
    "        window_size,\n",
    "        overlap,\n",
    "        params=None,\n",
    "    ):\n",
    "        self.input_audio_paths = input_audio_paths\n",
    "        self.target_audio_paths = target_audio_paths\n",
    "        self.window_size = window_size  # in seconds\n",
    "        self.window_size_samples = self.window_size * 44100 # in samples\n",
    "        self.overlap = overlap  # in seconds\n",
    "        self.overlap_samples = self.overlap * 44100 # in samples\n",
    "        self.params = params  # compressor parameters i.e compress-limit switch, peak reduction\n",
    "        self.window_to_audio_mapping = self.window_index_to_audio_indices() # map an\n",
    "        self.examples = self.create_examples()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset_size\n",
    "\n",
    "    def __getitem__(self, window_index):\n",
    "        \"\"\" \n",
    "        Converts a window index to a tuple of audio indices, i.e the first window (0)\n",
    "        corresponds to the first audio (0) and the first window in this audio (0). \n",
    "\n",
    "        The set of {window_index: (audio_index, window_index_in_audio)} is precomputed \n",
    "        in the `self.window_to_audio_mapping method`. \n",
    "        \n",
    "        We use this tuple of indices to \n",
    "        load the actual audio data (the stream of samples), lazily, for training. \n",
    "        \"\"\"\n",
    "        audio_index, window_index_in_audio = self.window_to_audio_mapping[window_index]\n",
    "        input_audio_path, target_audio_path, params = self.examples[audio_index]\n",
    "\n",
    "        input_waveform, _ = torchaudio.load(\n",
    "            input_audio_path,\n",
    "            num_frames=self.window_size_samples,\n",
    "            frame_offset=window_index_in_audio * self.overlap_samples,\n",
    "        ) \n",
    "\n",
    "        target_waveform, _ = torchaudio.load(\n",
    "            target_audio_path,\n",
    "            num_frames=self.window_size_samples,\n",
    "            frame_offset=window_index_in_audio * self.overlap_samples,\n",
    "        )\n",
    "        \n",
    "        input_waveform = input_waveform[0] # 1D tensor e.g [0.1, 0.03, ...] of length window size * sample rate\n",
    "        target_waveform = target_waveform[0]\n",
    "\n",
    "        input_waveform = (input_waveform + 1.0) / 2.0 # Rescale (invertibly) values that go from -1 to 1, to be between 0 and 1\n",
    "        target_waveform = (target_waveform + 1.0) / 2.0 \n",
    "\n",
    "        compress_limit = torch.tensor(params[0], dtype=torch.float32)\n",
    "        peak_reduction = torch.tensor(params[1]/100.0, dtype=torch.float32) # Ensure the peak reduction is between 0 and 1, not 0 - 100\n",
    "\n",
    "        return (input_waveform, compress_limit, peak_reduction), target_waveform\n",
    "\n",
    "    def window_index_to_audio_indices(self):\n",
    "        \"\"\" \n",
    "        Our audio dataset constructor takes in a list of input/target audio paths. In the SignalTrain dataset, each \n",
    "        individual audio is very long, up to 20 mins. Our model will have a much smaller input size. Therefore, we \n",
    "        will use a given audio file to create several training examples by taking overlapping windows of for example 3 seconds \n",
    "        from each file. e.g 0-3s, 1-4s, 2-5s all the way up until we use all 20 minutes of the audio file.\n",
    "\n",
    "        Since __getitem__ method is lazy, it will try to load each window of audio data on the fly instead of preparing\n",
    "        them in advance. This means that when we say __getitem__(293), we need to know which audio file corresponds to \n",
    "        window 293, and where precisely this window is situated in that audio file, so that we can load the right data.\n",
    "        \n",
    "        For example, window with index 0 can be identified with the first audio file, and the first window in that audio i.e (0,0).\n",
    "        \n",
    "        This function is used to pre-compute the mapping from a window index into a tuple of indices identifying both the audio file,\n",
    "        and the position of this window in that audio file so that we can load the audio data on the fly in __getitem__. \n",
    "        \"\"\"\n",
    "        cumulative_window_index = 0\n",
    "        mapping = {}\n",
    "        for audio_index, path in enumerate(self.input_audio_paths):\n",
    "            num_windows = self.calculate_num_windows(path)\n",
    "            for i in range(num_windows):\n",
    "                mapping[cumulative_window_index + i] = (\n",
    "                    audio_index,\n",
    "                    i,\n",
    "                )  # e.g (3rd audio, 4th window of 3rd audio)\n",
    "            cumulative_window_index += num_windows\n",
    "        self.dataset_size = len(mapping)\n",
    "\n",
    "        return mapping\n",
    "\n",
    "    def calculate_num_windows(self, audio_path):\n",
    "        waveform, sample_rate = torchaudio.load(audio_path)\n",
    "        window_samples = self.window_size * sample_rate\n",
    "        overlap_samples = self.overlap * sample_rate\n",
    "        num_windows = overlapping_interval_count(\n",
    "            waveform.size(1), window_samples, overlap_samples\n",
    "        )\n",
    "        return num_windows\n",
    "\n",
    "    def create_examples(self):\n",
    "        return list(zip(self.input_audio_paths, self.target_audio_paths, self.params))\n",
    "\n",
    "\n",
    "# Example usage with multiple paths and parameters\n",
    "train_df = df[df.split == \"train\"]\n",
    "test_df = df[df.split == \"test\"]\n",
    "val_df = df[df.split == \"val\"]\n",
    "\n",
    "train_dataset = AudioDataset(\n",
    "    train_df[\"raw_audio_path\"].to_list(),\n",
    "    train_df[\"processed_audio_path\"].to_list(),\n",
    "    window_size=0.1,\n",
    "    overlap=0.05,\n",
    "    params=list(zip(train_df[\"compress_or_limit\"], train_df[\"peak_reduction\"])),\n",
    ")\n",
    "test_dataset = AudioDataset(\n",
    "    test_df[\"raw_audio_path\"].to_list(),\n",
    "    test_df[\"processed_audio_path\"].to_list(),\n",
    "    window_size=0.1,\n",
    "    overlap=0.05,\n",
    "    params=list(zip(test_df[\"compress_or_limit\"], test_df[\"peak_reduction\"])),\n",
    ")\n",
    "val_dataset = AudioDataset(\n",
    "    val_df[\"raw_audio_path\"].to_list(),\n",
    "    val_df[\"processed_audio_path\"].to_list(),\n",
    "    window_size=0.1,\n",
    "    overlap=0.05,\n",
    "    params=list(zip(val_df[\"compress_or_limit\"], val_df[\"peak_reduction\"])),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: \n",
    "- Torchaudio loads audio as a tuple containing a 2D tensor and the sample rate. The 2D tensor is structured as (num_channels, num_frames). In the case of mono audio (1 channel), it still follows this format, but with the number of channels being 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an example of what our input / output data will look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input waveform:\n",
      "tensor([0.5295, 0.5309, 0.5323,  ..., 0.5065, 0.5098, 0.5129]), shape: torch.Size([4410])\n",
      "\n",
      "compress or limit:\n",
      "1.0, shape: torch.Size([])\n",
      "\n",
      "peak reduction:\n",
      "0.949999988079071, shape: torch.Size([])\n",
      "\n",
      "target waveform:\n",
      "tensor([0.5072, 0.5076, 0.5079,  ..., 0.5013, 0.5022, 0.5031]), shape: torch.Size([4410])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(input_waveform, cl, pr), target_waveform = train_dataset.__getitem__(400)\n",
    "\n",
    "print(f\"input waveform:\\n{input_waveform}, shape: {input_waveform.shape}\\n\")\n",
    "print(f\"compress or limit:\\n{cl}, shape: {cl.shape}\\n\")\n",
    "print(f\"peak reduction:\\n{pr}, shape: {pr.shape}\\n\")\n",
    "print(f\"target waveform:\\n{target_waveform}, shape: {target_waveform.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a fairly standard audio sequence modelling setup. We will use an LSTM to model the input audio sequence and obtain a sequence of the same length as the input sequence, but where for each time step we will have constructed a \"hidden representation\" i.e an input of length N will have an output of shape (N, H) where H is the hidden dimension of the LSTM. As for the compressor parameters, we will embed them into dense representations of size K, and then transpose them. We can then flatten our LSTM output to get a 1D tensor, concat it with our transposed compressor parameters and then pass all of this into a FFN to recover an output of length N (our target audio). Here's an example diagram:\n",
    "\n",
    "<img src=\"assets/sample-archi.png\" alt=\"Image Alt Text\" width=\"75%\">\n",
    "\n",
    "Assuming our audio has a sample rate of 44.1kHz and we use a window size of 3 seconds, we will be predicting 132300 samples (might be a lot).\n",
    "\n",
    "Later we'll try some other architectures.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = train_dataset.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.5000, 0.5000, 0.5000,  ..., 0.4828, 0.4836, 0.4846]),\n",
       " tensor(1.),\n",
       " tensor(0.9500))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4410])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for data, label in dataloader:\n",
    "    print(len(data))\n",
    "    print(data[0].shape)\n",
    "    print(data[1].shape)\n",
    "    print(data[2].shape)\n",
    "    print(label.shape)\n",
    "    raise TypeError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1306738\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Model\n",
      "lstm initialized\n",
      "compress embedding initialized\n",
      "pr embedding initialized\n",
      "input_size: 282496\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "import gc \n",
    "\n",
    "gc.collect()\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "# Hyperparameters\n",
    "sequence_length = 4410  # Assuming each audio interval has this length\n",
    "batch_size = 2\n",
    "hidden_size = 64  # Experiment with different values\n",
    "fixed_embedding_size = 128\n",
    "learning_rate = 0.001\n",
    "\n",
    "dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "class MLPDecoder(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        print(f\"input_size: {input_size}\")\n",
    "        super().__init__()\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.dense1 = nn.Linear(input_size, int(input_size/4))\n",
    "        self.dense2 = nn.Linear( int(input_size/4),  int(input_size/8))\n",
    "        self.dense3 = nn.Linear( int(input_size/8),  int(input_size/16))\n",
    "        self.dense4 = nn.Linear( int(input_size/16),  sequence_length)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"X : {x.shape}\")\n",
    "        x = self.relu1(self.dense1(x))\n",
    "        x = self.relu2(self.dense2(x))\n",
    "        x = self.relu3(self.dense3(x))\n",
    "        x = self.relu4(self.dense4(x))\n",
    "        return x\n",
    "    \n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size)\n",
    "        print(f\"lstm initialized\")\n",
    "        self.compress_switch_embedding = nn.Embedding(num_embeddings=1, embedding_dim=fixed_embedding_size)\n",
    "        print(f\"compress embedding initialized\")\n",
    "\n",
    "        self.peak_reduction_embedding = nn.Embedding(num_embeddings=1, embedding_dim=fixed_embedding_size)\n",
    "        print(f\"pr embedding initialized\")\n",
    "\n",
    "        self.decoder = MLPDecoder(sequence_length*hidden_size + 2*fixed_embedding_size, sequence_length)\n",
    "        print(f\"decoder initialized\")\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        print(f\"inputs: {len(inputs)}\")\n",
    "        lstm_input = inputs[0].to(device)\n",
    "        compress_limit_input = inputs[1].to(device).long()\n",
    "        peak_reduction_input = inputs[2].to(device).long()\n",
    "\n",
    "        # LSTM\n",
    "        input_seq = lstm_input.transpose(0, 1)\n",
    "        input_seq = input_seq.unsqueeze(2)\n",
    "\n",
    "        lstm_output, (h_n, c_n) = self.lstm(input_seq)\n",
    "        transposed_lstm_output = lstm_output.transpose(1,0)\n",
    "        flattened_lstm_output = torch.flatten(transposed_lstm_output, 1,2)\n",
    "\n",
    "        # Embed Compression Switch \n",
    "        embedded_compress_switch = self.compress_switch_embedding(compress_limit_input)\n",
    "\n",
    "        # Embed PR Parameter\n",
    "        embedded_peak_reduction = self.peak_reduction_embedding(peak_reduction_input)\n",
    "\n",
    "        # Concat all embeddings\n",
    "        concat_embeddings = torch.cat([flattened_lstm_output, embedded_compress_switch, embedded_peak_reduction], dim=1)\n",
    "\n",
    "        # Pass embeddings through decoder MLP \n",
    "        prediction = self.decoder(concat_embeddings)\n",
    "\n",
    "        return prediction\n",
    "\n",
    "\n",
    "print(f\"Before Model\")\n",
    "model = LSTMModel(1, hidden_size)  # Input size is 1 due to reshaping\n",
    "print(f\"Model initialized\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "print(f\"Optimizer initialized\")\n",
    "loss_fn = nn.MSELoss()  # Adjust loss function if needed\n",
    "print(f\"Loss fn initialized\")\n",
    "model.to(device)\n",
    "print(f\"Model on GPU: {model}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop\n",
    "for epoch in range(1):\n",
    "    for data, label in dataloader:\n",
    "        start_time = time.time()\n",
    "\n",
    "        label = label.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        prediction = model(data)\n",
    "        print(f\"YEAAA\")\n",
    "        loss = loss_fn(prediction, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        end_time = time.time()\n",
    "        print(f\"EPOCH {epoch+1} TIME: {(end_time - start_time)/60.0} mins\")\n",
    "        print(f\"EPOCH {epoch+1} LOSS: {loss.item()}\\n\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THE OVERLAP SAMPLES IS NOT CORRECT I THINK, IT IS A DECIMAL 44100 * 0.005 = 221.5 or so, it should probably be whole number.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fix up shapes to match what we actually use and then make this work with the dataset object"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
